{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Pipeline de traitement  processus de détection  un  langage abusif sur le Social Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1-Data Collection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We start by collecting Data Using Social web APIs (Facebook Graph API , Twitter API , Instagram API).\n",
    "In the detection of abusive language we are talking about a supervised learning problem. This means we need a labeled dataset so the algorithms can learn the patterns and correlations in the data .\n",
    "To label the data : \n",
    "     - Create rule based conditions based on list of bad words\n",
    "     - Data augmentation using open data bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2-Data Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* Dataset balancing :  undersample the majority class and oversample the minority one\n",
    "\n",
    "* Explore the collected data and perform a cleaning process to ensure no distortions :\n",
    "     - Clean stop words\n",
    "     - delete or transform special characters\n",
    "     - downcased every word\n",
    "     - stemming\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3-Feature engeneering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In natural language processing we can try : \n",
    "        - Bag of words and TFID (example in python : sklearn.feature_extraction.text has implemented CountVectorizer and TfidfTransformer )\n",
    "        - Word Embeddings (example in python : topic modeling tool \"gensim\" has implemented the word2vec)\n",
    "        - NLP based features like Part of Speech models (example in python : NLTK wich is a Natural Language Toolkit)\n",
    "        - Topic modeling (example in python : LatentDirichletAllocation in sklearn.decomposition)\n",
    "We can try to add :\n",
    "            - Ngrams features\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4-Train — test split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Split data in order to validate the model and prove it's quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5- Apply  machine learning models "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* test multiple text classifcation models wich will help us to figure out which one may fit better our  data :\n",
    "    - Random Forest\n",
    "    - Support Vector Machine\n",
    "    - K Nearest Neighbors\n",
    "    - Multinomial Naïve Bayes\n",
    "    - Logistic Regression\n",
    "    - Gradient Boosting\n",
    "* Hyperparameter tuning\n",
    "* Performance Measurement :\n",
    "    - Accuracy\n",
    "    - Use Area Under the ROC Curve (AUC) wich is used in  classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - Select Best Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Choose the model with the  evaluation metric and save it to be used in production (pickle file).\n",
    "For model deployment, the pickle file can be moved to the production site .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 - Adding new validated data and retrain the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Add new validated data to train our algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Expliquer selon toi comment nous pouvons utiliser un algo hybrid CNN-LSTM pour détection d'un langage abusif sur le Social Web"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* CNN  help us to extract local features from our texts  using a  convolutional layer and max-pooling layer ( by thee using of Convolutions and pooling operations we can drop  informations about words local order ) ,while  LSTM can help us extract contextual dependencies of texts by saving historical information. \n",
    "For abuse language detection from comments and posts the context is important.\n",
    "\n",
    "* To achieve excellent performance we can use a hybrid CNN-LSTM algorithm : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 feature creation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We start by using the word embedding method to represent each word in the text (we can use word2vec method implemented by topic modeling tool \"gensim\" ) . the output of this method is a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Apply CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Convolution Layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* The Matrix is used as an input of the CNN Model. The first layer is a convolution layer with filters . the filter is applied to a window of word sequence of H (width of filter ) and each word will be represented as  a vector of dimensionality N . After applying it we get a new feature . Then,we  slide the filter through the  text to generate a feature map based on all the word concatenation ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Pooling Layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* this layer will allow us to get  the most important semantic features P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> to apply CNN using python we can use keras  (convolution layer : keras.layers.Conv2D ,  Pooling Layer :keras.layers.MaxPool2D  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Apply LSTM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* we rearrange P1 features (Output of CNN)  with respect to the time sequence .The output will be used as the input of the LSTM model\n",
    "* Applying LSTM will generate new features P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==> to apply LSTM using python we can use keras  (convolution layer : keras.layers.CuDNNLSTM or  Pooling Layer :keras.layers.LSTM  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 combine the features P1, which is generated by CNN, and P2 which is generated by LSTM as the final features p. At last, the final features p acts as the input of a fully connected softmax layer. The output of the layer is the query classification distribution over all the categories. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
